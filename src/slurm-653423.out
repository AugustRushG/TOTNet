Number of GPUs: 1
GPU 0: NVIDIA L40S
2024-10-14 01:09:53,743: logger.py - info(), at Line 39:INFO:
>>> Created a new logger
2024-10-14 01:09:53,795: logger.py - info(), at Line 39:INFO:
>>> configs: {'seed': 2024, 'working_dir': '../', 'saved_fn': 'mutli_frames_masked_train_270_480_single', 'smooth_labelling': False, 'no_val': False, 'no_test': True, 'val_size': 0.2, 'num_samples': None, 'batch_size': 16, 'num_workers': 4, 'distributed': True, 'print_freq': 50, 'checkpoint_freq': 2, 'earlystop_patience': None, 'save_test_output': False, 'pretrained_path': None, 'backbone_choice': 'single', 'backbone_pretrained': True, 'backbone_out_channels': 2048, 'transfromer_dmodel': 512, 'transformer_nhead': 8, 'num_feature_levels': 1, 'num_classes': 1, 'num_queries': 50, 'video_path': None, 'output_format': 'text', 'show_image': False, 'save_demo_output': False, 'num_frames': 1, 'start_epoch': 1, 'num_epochs': 30, 'lr': 0.0001, 'minimum_lr': 1e-07, 'momentum': 0.9, 'weight_decay': 0.0, 'optimizer_type': 'adam', 'lr_type': 'plateau', 'lr_factor': 0.5, 'lr_step_size': 5, 'lr_patience': 3, 'img_size': [270, 480], 'world_size': 1, 'rank': 0, 'dist_url': 'tcp://127.0.0.1:29500', 'dist_backend': 'nccl', 'gpu_idx': 0, 'no_cuda': False, 'multiprocessing_distributed': True, 'device': device(type='cuda', index=0), 'ngpus_per_node': 1, 'pin_memory': True, 'org_size': (1080, 1920), 'results_dir': '../results', 'logs_dir': '../logs/mutli_frames_masked_train_270_480_single', 'checkpoints_dir': '../checkpoints/mutli_frames_masked_train_270_480_single', 'dataset_dir': '/home/s224705071/github/TT/TTNet-Real-time-Analysis-System-for-Table-Tennis-Pytorch/dataset', 'train_game_list': ['game_1', 'game_2', 'game_3', 'game_4', 'game_5'], 'test_game_list': ['test_1', 'test_2', 'test_3', 'test_4', 'test_5', 'test_6', 'test_7'], 'events_dict': {'bounce': 0, 'net': 1, 'empty_event': 2}, 'events_weights_loss_dict': {'bounce': 1.0, 'net': 3.0}, 'is_master_node': True}
2024-10-14 01:09:56,695: logger.py - info(), at Line 39:INFO:
number of trained parameters of the model: 65000377
2024-10-14 01:09:56,695: logger.py - info(), at Line 39:INFO:
>>> Loading dataset & getting dataloader...
2024-10-14 01:09:57,614: logger.py - info(), at Line 39:INFO:
number of batches in train set: 2259
2024-10-14 01:09:57,614: logger.py - info(), at Line 39:INFO:
number of batches in val set: 564
2024-10-14 01:09:57,614: logger.py - info(), at Line 39:INFO:
number of batches in test set: 428
2024-10-14 01:09:57,614: logger.py - info(), at Line 39:INFO:
*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-
2024-10-14 01:09:57,614: logger.py - info(), at Line 39:INFO:
=================================== 1/30 ===================================
2024-10-14 01:09:57,614: logger.py - info(), at Line 39:INFO:
*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-
2024-10-14 01:09:57,614: logger.py - info(), at Line 39:INFO:
>>> Epoch: [1/30] learning rate: 1.00e-04
Use GPU: 0 for training
../logs/mutli_frames_masked_train_270_480_single mutli_frames_masked_train_270_480_single
GPU 0 (Rank 0): 36147 samples total, 36147 samples for this GPU
  0%|          | 0/2259 [00:00<?, ?it/s]Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f37a6f81f70>
Traceback (most recent call last):
  File "/home/s224705071/.conda/envs/PIDA/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1477, in __del__
    self._shutdown_workers()
  File "/home/s224705071/.conda/envs/PIDA/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1441, in _shutdown_workers
    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)
  File "/home/s224705071/.conda/envs/PIDA/lib/python3.9/multiprocessing/process.py", line 149, in join
    res = self._popen.wait(timeout)
  File "/home/s224705071/.conda/envs/PIDA/lib/python3.9/multiprocessing/popen_fork.py", line 40, in wait
    if not wait([self.sentinel], timeout):
  File "/home/s224705071/.conda/envs/PIDA/lib/python3.9/multiprocessing/connection.py", line 931, in wait
    ready = selector.select(timeout)
  File "/home/s224705071/.conda/envs/PIDA/lib/python3.9/selectors.py", line 416, in select
    fd_event_list = self._selector.poll(timeout)
  File "/home/s224705071/.conda/envs/PIDA/lib/python3.9/site-packages/torch/utils/data/_utils/signal_handling.py", line 67, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 3267659) is killed by signal: Aborted. 
  0%|          | 0/2259 [00:09<?, ?it/s]
torch.Size([16, 0])
Traceback (most recent call last):
  File "/home/s224705071/github/PhysicsInformedDeformableAttentionNetwork/src/main.py", line 291, in <module>
    main()
  File "/home/s224705071/github/PhysicsInformedDeformableAttentionNetwork/src/main.py", line 53, in main
    mp.spawn(main_worker, nprocs=configs.ngpus_per_node, args=(configs,))
  File "/home/s224705071/.conda/envs/PIDA/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 282, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
  File "/home/s224705071/.conda/envs/PIDA/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 238, in start_processes
    while not context.join():
  File "/home/s224705071/.conda/envs/PIDA/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 189, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 0 terminated with the following error:
Traceback (most recent call last):
  File "/home/s224705071/.conda/envs/PIDA/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 76, in _wrap
    fn(i, *args)
  File "/home/s224705071/github/PhysicsInformedDeformableAttentionNetwork/src/main.py", line 140, in main_worker
    train_loss = train_one_epoch(train_loader, model, optimizer, loss_func, epoch, configs, logger)
  File "/home/s224705071/github/PhysicsInformedDeformableAttentionNetwork/src/main.py", line 209, in train_one_epoch
    output_coords = model(batch_data.float()) # output in shape ([B,W],[B,H]) if output heatmap
  File "/home/s224705071/.conda/envs/PIDA/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/s224705071/.conda/envs/PIDA/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/s224705071/.conda/envs/PIDA/lib/python3.9/site-packages/torch/nn/parallel/distributed.py", line 1636, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/home/s224705071/.conda/envs/PIDA/lib/python3.9/site-packages/torch/nn/parallel/distributed.py", line 1454, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/home/s224705071/.conda/envs/PIDA/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/s224705071/.conda/envs/PIDA/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/s224705071/github/PhysicsInformedDeformableAttentionNetwork/src/model/deformable_detection_model.py", line 103, in forward
    B, N, C, H, W = samples.shape
ValueError: not enough values to unpack (expected 5, got 2)

