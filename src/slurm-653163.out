Number of GPUs: 2
GPU 0: NVIDIA L40S
GPU 1: NVIDIA L40S
2024-10-08 10:25:59,414: logger.py - info(), at Line 39:INFO:
>>> Created a new logger
2024-10-08 10:25:59,429: logger.py - info(), at Line 39:INFO:
>>> configs: {'seed': 2024, 'working_dir': '../', 'saved_fn': 'normal_train_270_480', 'smooth_labelling': False, 'no_val': False, 'no_test': True, 'val_size': 0.2, 'num_samples': None, 'batch_size': 256, 'num_workers': 4, 'distributed': True, 'print_freq': 20, 'checkpoint_freq': 2, 'earlystop_patience': None, 'save_test_output': False, 'pretrained_path': None, 'backbone_choice': 'single', 'backbone_pretrained': True, 'backbone_out_channels': 2048, 'transfromer_dmodel': 512, 'transformer_nhead': 8, 'num_feature_levels': 1, 'num_classes': 1, 'num_queries': 10, 'video_path': None, 'output_format': 'text', 'show_image': False, 'save_demo_output': False, 'start_epoch': 1, 'num_epochs': 30, 'lr': 0.0001, 'minimum_lr': 1e-07, 'momentum': 0.9, 'weight_decay': 0.0, 'optimizer_type': 'adam', 'lr_type': 'plateau', 'lr_factor': 0.5, 'lr_step_size': 5, 'lr_patience': 3, 'img_size': [270, 480], 'world_size': 2, 'rank': 0, 'dist_url': 'tcp://127.0.0.1:29500', 'dist_backend': 'nccl', 'gpu_idx': 0, 'no_cuda': False, 'multiprocessing_distributed': True, 'device': device(type='cuda', index=0), 'ngpus_per_node': 2, 'pin_memory': True, 'org_size': (1080, 1920), 'num_frames_sequence': 1, 'results_dir': '../results', 'logs_dir': '../logs/normal_train_270_480', 'checkpoints_dir': '../checkpoints/normal_train_270_480', 'dataset_dir': '/home/s224705071/github/TT/TTNet-Real-time-Analysis-System-for-Table-Tennis-Pytorch/dataset', 'train_game_list': ['game_1', 'game_2', 'game_3', 'game_4', 'game_5'], 'test_game_list': ['test_1', 'test_2', 'test_3', 'test_4', 'test_5', 'test_6', 'test_7'], 'events_dict': {'bounce': 0, 'net': 1, 'empty_event': 2}, 'events_weights_loss_dict': {'bounce': 1.0, 'net': 3.0}, 'is_master_node': True}
2024-10-08 10:26:03,467: logger.py - info(), at Line 39:INFO:
number of trained parameters of the model: 64959417
2024-10-08 10:26:03,467: logger.py - info(), at Line 39:INFO:
>>> Loading dataset & getting dataloader...
2024-10-08 10:26:04,514: logger.py - info(), at Line 39:INFO:
number of batches in train set: 142
2024-10-08 10:26:04,514: logger.py - info(), at Line 39:INFO:
number of batches in val set: 35
2024-10-08 10:26:04,514: logger.py - info(), at Line 39:INFO:
number of batches in test set: 27
2024-10-08 10:26:04,514: logger.py - info(), at Line 39:INFO:
*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-
2024-10-08 10:26:04,514: logger.py - info(), at Line 39:INFO:
=================================== 1/30 ===================================
2024-10-08 10:26:04,514: logger.py - info(), at Line 39:INFO:
*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-
2024-10-08 10:26:04,514: logger.py - info(), at Line 39:INFO:
>>> Epoch: [1/30] learning rate: 1.00e-04
Use GPU: 0 for training
../logs/normal_train_270_480 normal_train_270_480
GPU 0 (Rank 0): 36147 samples total, 18074 samples for this GPU
  0%|          | 0/142 [00:00<?, ?it/s]Use GPU: 1 for training
GPU 1 (Rank 1): 36147 samples total, 18074 samples for this GPU
  0%|          | 0/142 [00:00<?, ?it/s]/home/s224705071/.conda/envs/PIDA/lib/python3.9/site-packages/torch/functional.py:513: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3609.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
/home/s224705071/.conda/envs/PIDA/lib/python3.9/site-packages/torch/functional.py:513: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3609.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
  0%|          | 0/142 [00:17<?, ?it/s]
  0%|          | 0/142 [00:17<?, ?it/s]
W1008 10:26:23.482893 140409061323392 torch/multiprocessing/spawn.py:146] Terminating process 1112388 via signal SIGTERM
Traceback (most recent call last):
  File "/home/s224705071/github/PhysicsInformedDeformableAttentionNetwork/src/main.py", line 291, in <module>
    main()
  File "/home/s224705071/github/PhysicsInformedDeformableAttentionNetwork/src/main.py", line 53, in main
    mp.spawn(main_worker, nprocs=configs.ngpus_per_node, args=(configs,))
  File "/home/s224705071/.conda/envs/PIDA/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 282, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method="spawn")
  File "/home/s224705071/.conda/envs/PIDA/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 238, in start_processes
    while not context.join():
  File "/home/s224705071/.conda/envs/PIDA/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 189, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 0 terminated with the following error:
Traceback (most recent call last):
  File "/home/s224705071/.conda/envs/PIDA/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 76, in _wrap
    fn(i, *args)
  File "/home/s224705071/github/PhysicsInformedDeformableAttentionNetwork/src/main.py", line 140, in main_worker
    train_loss = train_one_epoch(train_loader, model, optimizer, loss_func, epoch, configs, logger)
  File "/home/s224705071/github/PhysicsInformedDeformableAttentionNetwork/src/main.py", line 209, in train_one_epoch
    output_coords = model(batch_data.float()) # output in shape ([B,W],[B,H]) if output heatmap
  File "/home/s224705071/.conda/envs/PIDA/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/s224705071/.conda/envs/PIDA/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/s224705071/.conda/envs/PIDA/lib/python3.9/site-packages/torch/nn/parallel/distributed.py", line 1636, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/home/s224705071/.conda/envs/PIDA/lib/python3.9/site-packages/torch/nn/parallel/distributed.py", line 1454, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/home/s224705071/.conda/envs/PIDA/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/s224705071/.conda/envs/PIDA/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/s224705071/github/PhysicsInformedDeformableAttentionNetwork/src/model/deformable_detection_model.py", line 133, in forward
    x_coord_best = torch.gather(x_coord_logits, 1, best_query_indices.unsqueeze(-1).expand(-1, 1, x_coord_logits.shape[-1])).squeeze(1)  # Shape [B, w]
RuntimeError: The expanded size of the tensor (1) must match the existing size (128) at non-singleton dimension 1.  Target sizes: [-1, 1, 480].  Tensor sizes: [128, 1]

